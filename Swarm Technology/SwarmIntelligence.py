# -*- coding: utf-8 -*-
"""Deploy_to_huggingface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eh80fDxdv1pu4YeV26ciDt_tBu03CQbK
"""

!pip install transformers accelerate --quiet

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Model IDs
models_info = {
    "virtue_ethics": "sunidhisharma03/tiny-llama-virtue-ethics-lora",
    "deontology": "darshan012/llama-deontology",
    "utilitarian": "sunidhisharma03/tinyllama-utilitarian-lora"
}

# Load models and tokenizers
models = {}
tokenizers = {}

for name, model_id in models_info.items():
    print(f"Loading {name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    models[name] = model
    tokenizers[name] = tokenizer

print("‚úÖ All models loaded.")

def get_model_answer(question, model, tokenizer, max_new_tokens=100):
    prompt = f"""### Instruction:
{question}

### Response:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response_part = decoded_output.split("### Response:")[-1].strip()

    # Extract first Yes/No decision from response
    if "yes" in response_part.lower():
        return "Yes", response_part
    elif "no" in response_part.lower():
        return "No", response_part
    else:
        return "Unclear", response_part

from collections import Counter

def swarm_vote(question):
    votes = []
    responses = {}

    for name in models.keys():
        answer, full_response = get_model_answer(question, models[name], tokenizers[name])
        votes.append(answer)
        responses[name] = full_response

    # Majority vote
    vote_counts = Counter(votes)
    final_decision = vote_counts.most_common(1)[0][0]

    return final_decision, votes, responses

question = "Should a person steal medicine to save their child?"

final_decision, individual_votes, responses = swarm_vote(question)

print("\nüìä Swarm Intelligence Voting Results:")
print(f"Question: {question}")
print(f"\nIndividual Votes: {individual_votes}")
print(f"Final Decision: {final_decision}\n")

print("üìú Full Model Responses:")
for name, resp in responses.items():
    print(f"\n{name.upper()} MODEL RESPONSE:")
    print(resp)

test_questions = [
    "Is it morally permissible to tell a lie to save someone's life?",
    "Should a doctor assist a patient's suicide if it relieves immense suffering?",
    "Is it ethical to steal medicine to save your child‚Äôs life if you cannot afford it?",
    "Should a police officer always enforce unjust laws?",
    "Is it acceptable to break a promise to prevent harm to many people?",
    "Should you refuse to obey an immoral government order even if it means punishment?",
    "Is it morally wrong to torture one person to potentially save many lives?",
    "Should a journalist publish confidential information that could endanger national security?",
    "Is it ethical to sacrifice one innocent person to save five in a trolley dilemma?",
    "Is it always wrong to deceive others, even for a good cause?",
    "Should a soldier disobey an order to kill innocent civilians?",
    "Is it acceptable to perform an immoral act if it prevents a greater immoral act?",
    "Should you return a borrowed weapon to a dangerous person if they demand it back?",
    "Is it morally permissible to break the law to protect an innocent person?",
    "Is it wrong to cheat on a test even if it doesn‚Äôt harm anyone else?",
    "Should a person help a stranger in danger even at great personal risk?",
    "Is it acceptable to violate privacy for national security reasons?",
    "Should people be punished for crimes they haven't yet committed but are predicted to commit?",
    "Is it moral to kill one hostage to save the rest in a standoff situation?",
    "Should a nurse withhold a terminal diagnosis to spare a patient emotional pain?"
]
for question in test_questions:
    final_decision, individual_votes, responses = swarm_vote(question)
    print("="*100)
    print(f"Question: {question}")
    print(f"Votes: {individual_votes}")
    print("üìú Full Model Responses:")
    for name, resp in responses.items():
      print(f"\n{name.upper()} MODEL RESPONSE:")
      print(resp)
    print(f"Final Decision: {final_decision}\n")

"""# **ACTUAL SWARM TECHNOLOGY**"""

# ============================== SETUP ==============================
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import asyncio
from collections import defaultdict
import nest_asyncio
nest_asyncio.apply()

# ============================== LOAD MODELS ==============================
models_info = {
    "virtue_ethics": "sunidhisharma03/tiny-llama-virtue-ethics-lora",
    "deontology": "darshan012/llama-deontology",
    "utilitarian": "sunidhisharma03/tinyllama-utilitarian-lora"
}

models = {}
tokenizers = {}

print("üîÑ Loading models...")
for name, model_id in models_info.items():
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    models[name] = model
    tokenizers[name] = tokenizer
print("‚úÖ All models loaded.")

# ============================== AGENT FUNCTION ==============================
def get_model_answer_with_confidence(question, model, tokenizer):
    prompt = f"### Instruction:\n{question}\n\n### Response:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            return_dict_in_generate=True,
            output_scores=True
        )

    decoded = tokenizer.decode(output.sequences[0], skip_special_tokens=True)
    response = decoded.split("### Response:")[-1].strip()

    # Calculate confidence based on max softmax scores
    score_stack = torch.stack([score.softmax(dim=-1).max() for score in output.scores])
    confidence = score_stack.mean().item()

    if "yes" in response.lower():
        return "Yes", response, confidence
    elif "no" in response.lower():
        return "No", response, confidence
    else:
        return "Unclear", response, confidence

# ============================== ASYNC AGENT ==============================
async def agent_decision(name, question):
    model = models[name]
    tokenizer = tokenizers[name]
    loop = asyncio.get_event_loop()
    return name, await loop.run_in_executor(None, get_model_answer_with_confidence, question, model, tokenizer)

# ============================== SWARM VOTING ==============================
async def swarm_vote(question):
    tasks = [agent_decision(name, question) for name in models.keys()]
    results = await asyncio.gather(*tasks)

    votes = defaultdict(float)
    full_responses = {}
    confidences = {}

    for name, (vote, response, confidence) in results:
        votes[vote] += confidence
        full_responses[name] = response
        confidences[name] = (vote, round(confidence, 3))

    final_decision = max(votes.items(), key=lambda x: x[1])[0]
    return final_decision, confidences, full_responses

# ============================== TEST CASES ==============================
async def test_all_questions():
    test_questions = [
        "Should a person steal medicine to save their child?",
        "Should a doctor assist a patient's suicide if it relieves immense suffering?",
        "Should an employee leak confidential data to expose corruption?",
        "Should AI weapons be banned if they reduce human casualties?",
        "Should a company lie about eco-friendliness to prevent a market crash?"
    ]

    for question in test_questions:
        print("=" * 100)
        print(f"üß† Question: {question}")
        final_decision, confidence_votes, responses = await swarm_vote(question)

        print(f"\nüìä Final Swarm Decision: {final_decision}")
        print("\nü§ñ Individual Agent Votes:")
        for name, (vote, conf) in confidence_votes.items():
            print(f"  - {name}: {vote} (Confidence: {conf})")

        print("\nüìú Full Model Responses:")
        for name, response in responses.items():
            print(f"\n{name.upper()}:\n{response}")
        print("\n")

# ============================== RUN ==============================
await test_all_questions()

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import asyncio
import nest_asyncio
import gradio as gr
from collections import defaultdict

nest_asyncio.apply()

# ============================== LOAD MODELS ==============================
models_info = {
    "virtue_ethics": "sunidhisharma03/tiny-llama-virtue-ethics-lora",
    "deontology": "darshan012/llama-deontology",
    "utilitarian": "sunidhisharma03/tinyllama-utilitarian-lora"
}

models = {}
tokenizers = {}

print("üîÑ Loading models...")
for name, model_id in models_info.items():
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    models[name] = model
    tokenizers[name] = tokenizer
print("‚úÖ All models loaded.")

# ============================== AGENT DECISION FUNCTION ==============================
def get_model_answer_with_confidence(question, model, tokenizer):
    prompt = f"### Instruction:\n{question}\n\n### Response:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            return_dict_in_generate=True,
            output_scores=True
        )

    decoded = tokenizer.decode(output.sequences[0], skip_special_tokens=True)
    response = decoded.split("### Response:")[-1].strip()

    # Limit explanation to 2 sentences
    explanation = response.split(". ")
    explanation = ". ".join(explanation[:2]).strip()
    if not explanation.endswith("."):
        explanation += "."

    # Confidence: mean max-probability across generated tokens
    score_stack = torch.stack([score.softmax(dim=-1).max() for score in output.scores])
    confidence = score_stack.mean().item()

    if "yes" in response.lower():
        return "Yes", explanation, confidence
    elif "no" in response.lower():
        return "No", explanation, confidence
    else:
        return "Unclear", explanation, confidence

# ============================== ASYNC AGENT WRAPPER ==============================
async def agent_decision(name, question):
    model = models[name]
    tokenizer = tokenizers[name]
    loop = asyncio.get_event_loop()
    return name, await loop.run_in_executor(None, get_model_answer_with_confidence, question, model, tokenizer)

# ============================== SWARM VOTE FUNCTION ==============================
async def swarm_vote(question):
    tasks = [agent_decision(name, question) for name in models.keys()]
    results = await asyncio.gather(*tasks)

    vote_weights = defaultdict(float)
    explanations = {}
    confidences = {}
    votes = {}

    for name, (vote, explanation, confidence) in results:
        vote_weights[vote] += confidence
        explanations[name] = explanation
        confidences[name] = round(confidence, 3)
        votes[name] = vote

    final_decision = max(vote_weights.items(), key=lambda x: x[1])[0]
    return final_decision, votes, explanations, confidences

# ============================== GRADIO UI FUNCTION ==============================
def run_swarm(question):
    final_decision, votes, explanations, confidences = asyncio.run(swarm_vote(question))

    result_str = f"üß† **Swarm Final Decision:** `{final_decision}`\n\n"
    result_str += "### üó≥Ô∏è Individual Agent Responses:\n"
    for name in votes:
        result_str += f"**{name.title()}**:\n"
        result_str += f"- Vote: `{votes[name]}`\n"
        result_str += f"- Confidence: `{confidences[name]}`\n"
        result_str += f"- Explanation: {explanations[name]}\n\n"

    return result_str

# ============================== GRADIO APP ==============================
iface = gr.Interface(
    fn=run_swarm,
    inputs=gr.Textbox(lines=2, label="Enter an ethical dilemma/question"),
    outputs=gr.Markdown(label="Swarm Decision and Agent Responses"),
    title="üß† Swarm Ethics Decision Engine",
    description="This app uses 3 ethical LLM agents (Utilitarian, Deontology, Virtue) to vote on a Yes/No moral dilemma and explain their reasoning.",
    allow_flagging="never"
)

iface.launch(share=True)

# ============================== LOAD MODELS ==============================
models_info = {
    "virtue_ethics": "sunidhisharma03/tiny-llama-virtue-ethics-lora",
    "deontology": "darshan012/llama-deontology",
    "utilitarian": "sunidhisharma03/tinyllama-utilitarian-lora"
}

models = {}
tokenizers = {}

print("üîÑ Loading models...")
for name, model_id in models_info.items():
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    models[name] = model
    tokenizers[name] = tokenizer
print("‚úÖ All models loaded.")

# ============================== AGENT DECISION FUNCTION ==============================
def get_model_answer_with_confidence(question, model, tokenizer):
    prompt = f"### Instruction:\n{question}\n\n### Response:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            return_dict_in_generate=True,
            output_scores=True
        )

    decoded = tokenizer.decode(output.sequences[0], skip_special_tokens=True)
    response = decoded.split("### Response:")[-1].strip()

    # Limit explanation to 2 sentences
    explanation = response.split(". ")
    explanation = ". ".join(explanation[:3]).strip()
    if not explanation.endswith("."):
        explanation += "."

    # Confidence: mean max-probability across generated tokens
    score_stack = torch.stack([score.softmax(dim=-1).max() for score in output.scores])
    confidence = score_stack.mean().item()

    if "yes" in response.lower():
        return "Yes", explanation, confidence
    elif "no" in response.lower():
        return "No", explanation, confidence
    else:
        return "Unclear", explanation, confidence

# ============================== ASYNC AGENT WRAPPER ==============================
async def agent_decision(name, question):
    model = models[name]
    tokenizer = tokenizers[name]
    loop = asyncio.get_event_loop()
    return name, await loop.run_in_executor(None, get_model_answer_with_confidence, question, model, tokenizer)

# ============================== SWARM VOTE FUNCTION ==============================
async def swarm_vote(question):
    tasks = [agent_decision(name, question) for name in models.keys()]
    results = await asyncio.gather(*tasks)

    vote_weights = defaultdict(float)
    explanations = {}
    confidences = {}
    votes = {}

    for name, (vote, explanation, confidence) in results:
        vote_weights[vote] += confidence
        explanations[name] = explanation
        confidences[name] = round(confidence, 3)
        votes[name] = vote

    final_decision = max(vote_weights.items(), key=lambda x: x[1])[0]
    return final_decision, votes, explanations, confidences

# ============================== 3-ROUND SWARM LOGIC ==============================
async def run_swarm_async(question):
    round_results = []

    for round_num in range(1, 4):
        final_decision, votes, explanations, confidences = await swarm_vote(question)

        round_summary = f"### üîÅ Round {round_num} - Final Decision: `{final_decision}`\n"
        for name in votes:
            round_summary += f"**{name.title()}**: `{votes[name]}` (Confidence: {confidences[name]})\n"
            round_summary += f"‚û°Ô∏è {explanations[name]}\n\n"

        round_results.append((final_decision, round_summary))
        await asyncio.sleep(0.2)  # simulate thinking delay

    # Tally rounds
    final_votes = [result[0] for result in round_results]
    most_common = Counter(final_votes).most_common(1)[0][0]

    result_str = f"## üß† Overall Swarm Decision (After 3 Rounds): `{most_common}`\n\n"
    result_str += "\n---\n".join([r[1] for r in round_results])

    return result_str

# ============================== GRADIO UI ==============================
with gr.Blocks() as demo:
    gr.Markdown("# ü§ñ Swarm Ethics Decision Engine")
    gr.Markdown("Enter an ethical dilemma below. Three AI agents (Utilitarian, Deontology, Virtue) will vote **three times** and explain.")

    with gr.Row():
        question_input = gr.Textbox(label="üß† Ethical Dilemma", placeholder="e.g. Should a person lie to protect their friend?", lines=2)

    with gr.Row():
        submit_button = gr.Button("üó≥Ô∏è Get Swarm Decision (3 Rounds)")

    output_markdown = gr.Markdown(label="Results", visible=False)

    async def wrapper(question):
        output_markdown.visible = False
        await asyncio.sleep(0.2)  # show spinner
        result = await run_swarm_async(question)
        return gr.update(value=result, visible=True)

    submit_button.click(wrapper, inputs=[question_input], outputs=[output_markdown], show_progress="full")

demo.launch(share=True)

